<div align="center">
<br>
<h2>LLaDA Pretraining</h2>
<h3>Text Pretraining Framework</h3>
</div>

<p align="center">
  <a href="https://github.com/Gen-Verse/MMaDA">
    <img 
        src="https://img.shields.io/badge/Based%20on-MMaDA-green?logo=github&logoColor=white" 
        alt="Based on MMaDA"
    />
  </a>
  <a href="LICENSE">
    <img 
        src="https://img.shields.io/badge/License-MIT-yellow.svg" 
        alt="MIT License"
    />
  </a>
</p>

## ğŸŒŸ Introduction
Under testing...ï¼ˆç›®å‰è¿˜å±äºå›¢é˜Ÿè‡ªç”¨ï¼Œä¸Šä¼ ä¸Šæ¥çš„æ”¹åŠ¨äº†ä¸€éƒ¨åˆ†ï¼Œå¯èƒ½æœ‰å°‘è®¸bugï¼Œæ­£åœ¨æµ‹è¯•ä¸­ï¼‰
This is a text pretraining framework for LLaDA models, modified from the [MMaDA](https://github.com/Gen-Verse/MMaDA) codebase.

**Features:**
- Text-only training pipeline
- Distributed training support with DeepSpeed and Accelerate
- YAML-based configuration
- Memory efficient training options

## ğŸš€ Quick Start

### Envï¼ˆSource from MMaDAï¼‰
```bash
pip install -r requirements.txt
```

### Basic Training
```bash
# Update paths in configs/llada_pretraining.yaml
bash scripts/train.sh
```

## âš™ï¸ Configuration

Edit `configs/llada_pretraining.yaml`:

```yaml
model:
    pretrained_model_path: ".../LLaDA-8B-Base/"
    # LLaDA specific configuration
    llada_config:
        gradient_checkpointing: false  # close gradient checkpointing
        new_vocab_size: 126464
        # Add other LLaDA specific configs here if needed

dataset:
  params:
    train_shards_path_or_url: "path/to/data"
    
training:
  batch_size: 16
  max_train_steps: 100000
  mixed_precision: "bf16"
```

## ğŸ”§ Training

### Setup Accelerate
```bash
accelerate config
```

You can also use the provided configuration files in `accelerate_configs/` for different hardware and distributed setups:
- `1_gpu.yaml` - Single GPU
- `1_node_only.yaml` - Single node, single process (CPU or GPU)
- `1_node_8_gpus_deepspeed_zero1.yaml` - 8 GPUs with DeepSpeed ZeRO-1
- `1_node_8_gpus_deepspeed_zero2.yaml` - 8 GPUs with DeepSpeed ZeRO-2
- `1_node_8_gpus_deepspeed_zero3.yaml` - 8 GPUs with DeepSpeed ZeRO-3
- `8_node_8_gpus_deepspeed_zero2.yaml` - 8 nodes, each with 8 GPUs, DeepSpeed ZeRO-2

### Run Training
```bash
accelerate launch \
    --config_file accelerate_configs/1_node_8_gpus_deepspeed_zero1.yaml \
    --main_process_port=8888 \
    training/train_llada.py \
    config=configs/llada_pretraining.yaml
```

## ğŸ“ Project Structure

```
LLaDA_pretraining/
â”œâ”€â”€ accelerate_configs/     # Accelerate configurations
â”œâ”€â”€ configs/               # Training configurations
â”œâ”€â”€ models/               # Model implementations
â”œâ”€â”€ parquet/              # Data loading utilities
â”œâ”€â”€ training/             # Training scripts
â””â”€â”€ scripts/              # Shell scripts
```

## ğŸ› ï¸ Data Format
The files under the folder path you provided should be in JSONL format. It is recommended that the dataset be evenly split into multiple files with the number of files greater than the number of GPUs.
```json
{"text": "Training text content"}
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

Based on [MMaDA](https://github.com/Gen-Verse/MMaDA) by Yang et al.
